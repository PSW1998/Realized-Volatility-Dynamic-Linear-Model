{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ac0f22-2e23-4d12-a6d7-c536f99df9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/28jfj19121qf32rdzd8twc000000gn/T/ipykernel_5807/3612793093.py:30: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2025-01-01\", interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RVDLM.rvdlm_univariate symbols:\n",
      "['DynamicGammaFilter', 'RVDLM_Univariate', 'np', 'pd', 'student_t']\n",
      "\n",
      "RVDLM.tuning symbols:\n",
      "['RVDLM_Univariate', 'np', 'pd', 'tune_rvdlm_ohlc']\n",
      "\n",
      "RVDLM.dlm_baseline symbols:\n",
      "['dlm_lag1_logpred', 'grid_search_dlm', 'np', 'student_t']\n",
      "\n",
      "Head of working DataFrame:\n",
      "               Close       Vol     precision     percision\n",
      "Date                                                      \n",
      "2000-01-03  4.520569  0.000897   1114.439248   1114.439248\n",
      "2000-01-04  4.480677  0.000969   1031.999491   1031.999491\n",
      "2000-01-05  4.482464  0.000719   1391.132618   1391.132618\n",
      "2000-01-06  4.466262  0.000721   1386.154441   1386.154441\n",
      "2000-01-07  4.522715  0.000071  14087.750315  14087.750315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RV–DLM hyperparameters (from tune_rvdlm_ohlc):\n",
      "  beta_gamma   = 0.7500\n",
      "  alpha_gamma  = 2.0000\n",
      "  lambda_theta = 0.9900\n",
      "  Max log-lik  = 20555.15\n",
      "Recomputed RV–DLM log-likelihood: 20555.15\n",
      "\n",
      "Best classical DLM hyperparameters:\n",
      "  lambda_theta = 0.9900\n",
      "  lambda_sigma = 0.9000\n",
      "  Max log-lik  = 20311.36\n",
      "Recomputed classical DLM log-likelihood: 20311.36\n",
      "\n",
      "RV–DLM vs classical DLM comparison:\n",
      "  Δ log-lik (RV–DLM - DLM)     = 243.79\n",
      "  Per-observation Δ log-lik     = 0.038784\n",
      "  Per-observation BF (exp(Δ))   = 1.039546\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "import RVDLM\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 0. Inspect available submodules / functions\n",
    "# ------------------------------------------------------\n",
    "rvu = RVDLM.rvdlm_univariate\n",
    "tune = RVDLM.tuning\n",
    "dlm = getattr(RVDLM, \"dlm_baseline\", None)  # may be None if not imported in __init__\n",
    "\n",
    "print(\"RVDLM.rvdlm_univariate symbols:\")\n",
    "print([name for name in dir(rvu) if not name.startswith(\"_\")])\n",
    "\n",
    "print(\"\\nRVDLM.tuning symbols:\")\n",
    "print([name for name in dir(tune) if not name.startswith(\"_\")])\n",
    "\n",
    "if dlm is not None:\n",
    "    print(\"\\nRVDLM.dlm_baseline symbols:\")\n",
    "    print([name for name in dir(dlm) if not name.startswith(\"_\")])\n",
    "else:\n",
    "    print(\"\\nNo dlm_baseline module exposed via RVDLM.__init__ (baseline will be skipped).\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Download SPY daily data (2000–2025)\n",
    "# ------------------------------------------------------\n",
    "data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2025-01-01\", interval=\"1d\")\n",
    "\n",
    "# Drop Ticker level if yfinance returns a MultiIndex\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = data.columns.droplevel(\"Ticker\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Build log prices, RS realized variance, and precision\n",
    "# ------------------------------------------------------\n",
    "ohlc_log = np.log(data[[\"Open\", \"High\", \"Low\", \"Close\"]]).copy()\n",
    "\n",
    "# Rogers–Satchell realized variance (on log prices)\n",
    "rs_var = (\n",
    "    (ohlc_log[\"High\"] - ohlc_log[\"Open\"]) * (ohlc_log[\"High\"] - ohlc_log[\"Close\"])\n",
    "    + (ohlc_log[\"Open\"] - ohlc_log[\"Low\"]) * (ohlc_log[\"High\"] - ohlc_log[\"Low\"])\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(index=ohlc_log.index)\n",
    "df[\"Close\"] = ohlc_log[\"Close\"]\n",
    "df[\"Vol\"] = rs_var\n",
    "df[\"precision\"] = 1.0 / df[\"Vol\"]\n",
    "\n",
    "# Your earlier code used the typo 'percision', so mirror it in case you reuse old functions later\n",
    "df[\"percision\"] = df[\"precision\"]\n",
    "\n",
    "# Clean up infinities / NaNs, and keep only positive precision\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "df = df[df[\"precision\"] > 0]\n",
    "\n",
    "print(\"\\nHead of working DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Priors for RV–DLM and classical DLM\n",
    "# ------------------------------------------------------\n",
    "# RV–DLM state: [intercept, lag1, (optionally) leverage term]\n",
    "# Your current RVDLM_Univariate._design_vector uses:\n",
    "#   - with use_leverage=False: [1, y_{t-1}]\n",
    "#   - with use_leverage=True : [1, y_{t-1}, x_t]\n",
    "# We'll start with use_leverage=False for simplicity (2-dim state).\n",
    "use_leverage = False\n",
    "\n",
    "if use_leverage:\n",
    "    m0_rv = np.array([0.0, 0.9, 0.0])\n",
    "    C0_rv = np.eye(3) * 0.01\n",
    "else:\n",
    "    m0_rv = np.array([0.0, 0.9])\n",
    "    C0_rv = np.eye(2) * 0.01\n",
    "\n",
    "# Gamma / dynamic-F variance block initialization\n",
    "n0, s0 = 49.0, 1.0 / 5000.0\n",
    "\n",
    "# Classical DLM priors: state [intercept, lag1]\n",
    "m0_dlm = np.array([0.0, 0.9])\n",
    "C0_dlm = np.eye(2) * 0.01\n",
    "nu0, S0 = 2.0, 0.001\n",
    "\n",
    "y = df[\"Close\"].values\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Tune the RV–DLM via tune_rvdlm_ohlc\n",
    "# ------------------------------------------------------\n",
    "best_rv_params, best_rv_nll = tune.tune_rvdlm_ohlc(\n",
    "    df=df,\n",
    "    m0=m0_rv,\n",
    "    C0=C0_rv,\n",
    "    n0=n0,\n",
    "    s0=s0,\n",
    "    beta_grid=None,    # use defaults inside tune_rvdlm_ohlc\n",
    "    alpha_grid=None,\n",
    "    lambda_grid=None,\n",
    "    use_leverage=use_leverage,\n",
    ")\n",
    "\n",
    "beta_gamma, alpha_gamma, lambda_theta = best_rv_params\n",
    "\n",
    "print(\"\\nBest RV–DLM hyperparameters (from tune_rvdlm_ohlc):\")\n",
    "print(f\"  beta_gamma   = {beta_gamma:.4f}\")\n",
    "print(f\"  alpha_gamma  = {alpha_gamma:.4f}\")\n",
    "print(f\"  lambda_theta = {lambda_theta:.4f}\")\n",
    "print(f\"  Max log-lik  = {-best_rv_nll:.2f}\")\n",
    "\n",
    "# Build the best RV–DLM model explicitly and recompute log-likelihood\n",
    "rv_model = rvu.RVDLM_Univariate(\n",
    "    beta=beta_gamma,\n",
    "    alpha=alpha_gamma,\n",
    "    lambda_theta=lambda_theta,\n",
    "    m0=m0_rv,\n",
    "    C0=C0_rv,\n",
    "    n0=n0,\n",
    "    s0=s0,\n",
    "    use_leverage=use_leverage,\n",
    ")\n",
    "\n",
    "rv_loglik = rv_model.loglik(df)\n",
    "print(f\"Recomputed RV–DLM log-likelihood: {rv_loglik:.2f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Tune the classical DLM baseline (if dlm_baseline is available)\n",
    "# ------------------------------------------------------\n",
    "if dlm is not None and hasattr(dlm, \"grid_search_dlm\"):\n",
    "    best_dlm_params, best_dlm_nll = dlm.grid_search_dlm(\n",
    "        y=y,\n",
    "        m0=m0_dlm,\n",
    "        C0=C0_dlm,\n",
    "        nu0=nu0,\n",
    "        S0=S0,\n",
    "        lambda_theta_grid=None,   # use defaults inside grid_search_dlm\n",
    "        lambda_sigma_grid=None,\n",
    "    )\n",
    "    best_lambda_theta, best_lambda_sigma = best_dlm_params\n",
    "\n",
    "    print(\"\\nBest classical DLM hyperparameters:\")\n",
    "    print(f\"  lambda_theta = {best_lambda_theta:.4f}\")\n",
    "    print(f\"  lambda_sigma = {best_lambda_sigma:.4f}\")\n",
    "    print(f\"  Max log-lik  = {-best_dlm_nll:.2f}\")\n",
    "\n",
    "    # Recompute baseline log-likelihood using dlm_lag1_logpred\n",
    "    ll_dlm = dlm.dlm_lag1_logpred(\n",
    "        y=y,\n",
    "        lambda_theta=best_lambda_theta,\n",
    "        lambda_sigma=best_lambda_sigma,\n",
    "        m0=m0_dlm,\n",
    "        C0=C0_dlm,\n",
    "        nu0=nu0,\n",
    "        S0=S0,\n",
    "    )\n",
    "    dlm_loglik = np.sum(ll_dlm[1:])  # drop t=0\n",
    "\n",
    "    print(f\"Recomputed classical DLM log-likelihood: {dlm_loglik:.2f}\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 6. Compare RV–DLM vs classical DLM (total and per-observation)\n",
    "    # ------------------------------------------------------\n",
    "    n_obs = len(y) - 1  # effective number of one-step forecasts\n",
    "    delta_loglik = rv_loglik - dlm_loglik\n",
    "    per_obs_delta = delta_loglik / n_obs\n",
    "    per_obs_BF = np.exp(per_obs_delta)\n",
    "\n",
    "    print(\"\\nRV–DLM vs classical DLM comparison:\")\n",
    "    print(f\"  Δ log-lik (RV–DLM - DLM)     = {delta_loglik:.2f}\")\n",
    "    print(f\"  Per-observation Δ log-lik     = {per_obs_delta:.6f}\")\n",
    "    print(f\"  Per-observation BF (exp(Δ))   = {per_obs_BF:.6f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo classical DLM baseline available via RVDLM.dlm_baseline; \"\n",
    "          \"skipping baseline comparison.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
